{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/poligloteval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"projecte-aina/aguila-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split('/')[1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac21dd30-0551-43f9-8d8a-5b04df3a59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 53635.60it/s]\n"
     ]
    }
   ],
   "source": [
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 1\n",
    "    with torch.no_grad():\n",
    "        for token in answer_tokens:\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # Logits are in the outputs, you can access the last token's logits like this:\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            #log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "            log_probs = min_max_scaling(logits)\n",
    "            answer_probability *= log_probs[0][token]\n",
    "\n",
    "            # Prepare input_ids for the next token prediction\n",
    "            new_token = torch.tensor([[token]]).to(model.device)\n",
    "            inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                    'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "    #return torch.exp(answer_probability).item()\n",
    "    return answer_probability.item()\n",
    "\n",
    "\n",
    "def run_inference(txt, num_tokens=20, stop_text='\\n'):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    # Calculate the total length of the output (input length + number of tokens to generate)\n",
    "    max_length = len(inputs['input_ids'][0]) + num_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens\n",
    "        tokens = model.generate(**inputs, do_sample=True, top_k=1, eos_token_id=tokenizer.eos_token_id, max_length=max_length)\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Slice the generated text to exclude the input prompt\n",
    "        generated_only = generated_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):]\n",
    "\n",
    "        # If a stop text is found, truncate the output at its first occurrence\n",
    "        if stop_text in generated_only:\n",
    "            generated_only = generated_only.split(stop_text)[0]\n",
    "\n",
    "        return generated_only.strip()\n",
    "\n",
    "\n",
    "def translate(sample):\n",
    "    def translate_to_english(txt):\n",
    "        lines = txt.split(\"\\n\")\n",
    "        toks, _ = tokenizer_ca_en.tokenize_batch(lines)\n",
    "        translated = ca_en_model.translate_batch(toks)\n",
    "        ts = []\n",
    "        for t in translated:\n",
    "            ts.append(tokenizer_ca_en.detokenize(t.hypotheses[0]))\n",
    "\n",
    "        return \"\\n\".join(ts)\n",
    "    en_prompt = translate_to_english(sample['prompt'])\n",
    "    en_answer = translate_to_english(sample['answer'])\n",
    "    return {\"prompt\": en_prompt, \"answer\": en_answer}\n",
    "\n",
    "\n",
    "def compute_metrics(sample):\n",
    "    prob = compute_probability(sample['prompt'], sample['answer'])\n",
    "    prediction = run_inference(sample['prompt'])\n",
    "    f1 = f1_score(prediction, sample['answer'])\n",
    "    bleu = calculate_bleu_score(prediction, sample['answer'])\n",
    "    return {\"prediction\": prediction, \"prob\": prob, \"f1\": f1, \"bleu\": bleu}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d3a1bd-55a2-464a-acec-77d1aeb7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xquad_ca = load_dataset(\"data\", data_files=\"xquad_ca.csv\", split=\"train[:10]\")\n",
    "xquad_en = load_dataset(\"data\", data_files=\"xquad_en.csv\", split=\"train[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f9f86e-25f9-4544-9a35-209cff2edf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:18<00:00,  1.81s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob</th>\n",
       "      <th>f1</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"L'Estat Islàmic\", anteriorment conegut com a ...</td>\n",
       "      <td>Després de deixar la companyia d'Edison, Tesla...</td>\n",
       "      <td>militant extremista gihadista wahhabita/salafista</td>\n",
       "      <td>----</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"L'Estat Islàmic\", anteriorment conegut com a ...</td>\n",
       "      <td>Els himnes de Luter eren sovint evocats per co...</td>\n",
       "      <td>àrabs sunnites</td>\n",
       "      <td>----</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"L'Estat Islàmic\", anteriorment conegut com a ...</td>\n",
       "      <td>L'emperador Gegeen Kan, fill i successor d'Ayu...</td>\n",
       "      <td>deu milions</td>\n",
       "      <td>----</td>\n",
       "      <td>0.400391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"L'Estat Islàmic\", anteriorment conegut com a ...</td>\n",
       "      <td>La defensa dels Panthers va cedir només 308 pu...</td>\n",
       "      <td>reconeixement</td>\n",
       "      <td>----</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"L'Estat Islàmic\", anteriorment conegut com a ...</td>\n",
       "      <td>Posteriorment, es va trobar un tros de paper e...</td>\n",
       "      <td>califat</td>\n",
       "      <td>----</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Europa, el teatre nord-americà de la guerra ...</td>\n",
       "      <td>Els paleoclimatòlegs mesuren la proporció d’ox...</td>\n",
       "      <td>1756 fins a la signatura del tractat de pau el...</td>\n",
       "      <td>----</td>\n",
       "      <td>0.361328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Europa, el teatre nord-americà de la guerra ...</td>\n",
       "      <td>La defensa dels Panthers va cedir només 308 pu...</td>\n",
       "      <td>sis anys</td>\n",
       "      <td>----</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A Europa, el teatre nord-americà de la guerra ...</td>\n",
       "      <td>El 1466, potser 40.000 persones van morir a Pa...</td>\n",
       "      <td>el 1760</td>\n",
       "      <td>----</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Europa, el teatre nord-americà de la guerra ...</td>\n",
       "      <td>Peyton Manning es va convertir en el primer qu...</td>\n",
       "      <td>batalla de Jumonville Glen</td>\n",
       "      <td>----</td>\n",
       "      <td>0.462891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A causa de tenir cossos tous i gelatinosos, el...</td>\n",
       "      <td>El 1891, el químic escocès James Dewar va acon...</td>\n",
       "      <td>tenir cossos tous i gelatinosos</td>\n",
       "      <td>----</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  \"L'Estat Islàmic\", anteriorment conegut com a ...   \n",
       "1  \"L'Estat Islàmic\", anteriorment conegut com a ...   \n",
       "2  \"L'Estat Islàmic\", anteriorment conegut com a ...   \n",
       "3  \"L'Estat Islàmic\", anteriorment conegut com a ...   \n",
       "4  \"L'Estat Islàmic\", anteriorment conegut com a ...   \n",
       "5  A Europa, el teatre nord-americà de la guerra ...   \n",
       "6  A Europa, el teatre nord-americà de la guerra ...   \n",
       "7  A Europa, el teatre nord-americà de la guerra ...   \n",
       "8  A Europa, el teatre nord-americà de la guerra ...   \n",
       "9  A causa de tenir cossos tous i gelatinosos, el...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Després de deixar la companyia d'Edison, Tesla...   \n",
       "1  Els himnes de Luter eren sovint evocats per co...   \n",
       "2  L'emperador Gegeen Kan, fill i successor d'Ayu...   \n",
       "3  La defensa dels Panthers va cedir només 308 pu...   \n",
       "4  Posteriorment, es va trobar un tros de paper e...   \n",
       "5  Els paleoclimatòlegs mesuren la proporció d’ox...   \n",
       "6  La defensa dels Panthers va cedir només 308 pu...   \n",
       "7  El 1466, potser 40.000 persones van morir a Pa...   \n",
       "8  Peyton Manning es va convertir en el primer qu...   \n",
       "9  El 1891, el químic escocès James Dewar va acon...   \n",
       "\n",
       "                                              answer prediction      prob  f1  \\\n",
       "0  militant extremista gihadista wahhabita/salafista       ----  0.207031   0   \n",
       "1                                     àrabs sunnites       ----  0.535156   0   \n",
       "2                                        deu milions       ----  0.400391   0   \n",
       "3                                      reconeixement       ----  0.492188   0   \n",
       "4                                            califat       ----  0.419922   0   \n",
       "5  1756 fins a la signatura del tractat de pau el...       ----  0.361328   0   \n",
       "6                                           sis anys       ----  0.382812   0   \n",
       "7                                            el 1760       ----  0.488281   0   \n",
       "8                         batalla de Jumonville Glen       ----  0.462891   0   \n",
       "9                    tenir cossos tous i gelatinosos       ----  0.304688   0   \n",
       "\n",
       "   bleu  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "5     0  \n",
       "6     0  \n",
       "7     0  \n",
       "8     0  \n",
       "9     0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca = xquad_ca.map(compute_metrics)\n",
    "results_ca.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a429419-652f-4d6e-a2c1-a3161f2a13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:19<00:00,  1.92s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob</th>\n",
       "      <th>f1</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After leaving Edison's company Tesla partnered...</td>\n",
       "      <td>After leaving Edison's company Tesla partnered...</td>\n",
       "      <td>Wahhabi/Salafi jihadist extremist militant</td>\n",
       "      <td>----</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luther's hymns were frequently evoked by parti...</td>\n",
       "      <td>Luther's hymns were frequently evoked by parti...</td>\n",
       "      <td>Sunni Arabs</td>\n",
       "      <td>----</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emperor Gegeen Khan, Ayurbarwada's son and suc...</td>\n",
       "      <td>Emperor Gegeen Khan, Ayurbarwada's son and suc...</td>\n",
       "      <td>ten million</td>\n",
       "      <td>----</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Panthers defense gave up just 308 points, ...</td>\n",
       "      <td>The Panthers defense gave up just 308 points, ...</td>\n",
       "      <td>recognition</td>\n",
       "      <td>----</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A piece of paper was later found on which Luth...</td>\n",
       "      <td>A piece of paper was later found on which Luth...</td>\n",
       "      <td>a caliphate</td>\n",
       "      <td>----</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Paleoclimatologists measure the ratio of oxyge...</td>\n",
       "      <td>Paleoclimatologists measure the ratio of oxyge...</td>\n",
       "      <td>1756 to the signing of the peace treaty in 1763</td>\n",
       "      <td>----</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Panthers defense gave up just 308 points, ...</td>\n",
       "      <td>The Panthers defense gave up just 308 points, ...</td>\n",
       "      <td>six years</td>\n",
       "      <td>----</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In 1466, perhaps 40,000 people died of the pla...</td>\n",
       "      <td>In 1466, perhaps 40,000 people died of the pla...</td>\n",
       "      <td>1760</td>\n",
       "      <td>----</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Peyton Manning became the first quarterback ev...</td>\n",
       "      <td>Peyton Manning became the first quarterback ev...</td>\n",
       "      <td>Battle of Jumonville Glen</td>\n",
       "      <td>----</td>\n",
       "      <td>0.582031</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In 1891 Scottish chemist James Dewar was able ...</td>\n",
       "      <td>In 1891 Scottish chemist James Dewar was able ...</td>\n",
       "      <td>their soft, gelatinous bodies</td>\n",
       "      <td>----</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  After leaving Edison's company Tesla partnered...   \n",
       "1  Luther's hymns were frequently evoked by parti...   \n",
       "2  Emperor Gegeen Khan, Ayurbarwada's son and suc...   \n",
       "3  The Panthers defense gave up just 308 points, ...   \n",
       "4  A piece of paper was later found on which Luth...   \n",
       "5  Paleoclimatologists measure the ratio of oxyge...   \n",
       "6  The Panthers defense gave up just 308 points, ...   \n",
       "7  In 1466, perhaps 40,000 people died of the pla...   \n",
       "8  Peyton Manning became the first quarterback ev...   \n",
       "9  In 1891 Scottish chemist James Dewar was able ...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  After leaving Edison's company Tesla partnered...   \n",
       "1  Luther's hymns were frequently evoked by parti...   \n",
       "2  Emperor Gegeen Khan, Ayurbarwada's son and suc...   \n",
       "3  The Panthers defense gave up just 308 points, ...   \n",
       "4  A piece of paper was later found on which Luth...   \n",
       "5  Paleoclimatologists measure the ratio of oxyge...   \n",
       "6  The Panthers defense gave up just 308 points, ...   \n",
       "7  In 1466, perhaps 40,000 people died of the pla...   \n",
       "8  Peyton Manning became the first quarterback ev...   \n",
       "9  In 1891 Scottish chemist James Dewar was able ...   \n",
       "\n",
       "                                            answer prediction      prob  f1  \\\n",
       "0       Wahhabi/Salafi jihadist extremist militant       ----  0.251953   0   \n",
       "1                                      Sunni Arabs       ----  0.390625   0   \n",
       "2                                      ten million       ----  0.386719   0   \n",
       "3                                      recognition       ----  0.419922   0   \n",
       "4                                      a caliphate       ----  0.447266   0   \n",
       "5  1756 to the signing of the peace treaty in 1763       ----  0.235352   0   \n",
       "6                                        six years       ----  0.414062   0   \n",
       "7                                             1760       ----  0.531250   0   \n",
       "8                        Battle of Jumonville Glen       ----  0.582031   0   \n",
       "9                    their soft, gelatinous bodies       ----  0.339844   0   \n",
       "\n",
       "   bleu  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "5     0  \n",
       "6     0  \n",
       "7     0  \n",
       "8     0  \n",
       "9     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_en = xquad_en.map(compute_metrics)\n",
    "results_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 234.94ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 294.46ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18607"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca.to_csv(f\"results/{model_name}-xquad-ca.csv\", index=False)\n",
    "results_en.to_csv(f\"results/{model_name}-xquad-en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob    0.405469\n",
      "f1      0.000000\n",
      "bleu    0.000000\n",
      "dtype: float64\n",
      "prob    0.399902\n",
      "f1      0.000000\n",
      "bleu    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_ca_mean = results_ca.to_pandas()[['prob', 'f1', 'bleu']].mean()\n",
    "results_en_mean = results_en.to_pandas()[['prob', 'f1', 'bleu']].mean()\n",
    "print(results_ca_mean)\n",
    "print(results_en_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
