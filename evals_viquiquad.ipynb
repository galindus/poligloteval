{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:11<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"projecte-aina/aguila-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split('/')[1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac21dd30-0551-43f9-8d8a-5b04df3a59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 27666.91it/s]\n"
     ]
    }
   ],
   "source": [
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 1\n",
    "    with torch.no_grad():\n",
    "        for token in answer_tokens:\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # Logits are in the outputs, you can access the last token's logits like this:\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            #log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "            log_probs = min_max_scaling(logits)\n",
    "            answer_probability *= log_probs[0][token]\n",
    "\n",
    "            # Prepare input_ids for the next token prediction\n",
    "            new_token = torch.tensor([[token]]).to(model.device)\n",
    "            inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                    'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "    #return torch.exp(answer_probability).item()\n",
    "    return answer_probability.item()\n",
    "\n",
    "\n",
    "def run_inference(txt, num_tokens=20, stop_text='\\n'):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    # Calculate the total length of the output (input length + number of tokens to generate)\n",
    "    max_length = len(inputs['input_ids'][0]) + num_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens\n",
    "        tokens = model.generate(**inputs, do_sample=True, top_k=1, eos_token_id=tokenizer.eos_token_id, max_length=max_length)\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Slice the generated text to exclude the input prompt\n",
    "        generated_only = generated_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):]\n",
    "\n",
    "        # If a stop text is found, truncate the output at its first occurrence\n",
    "        if stop_text in generated_only:\n",
    "            generated_only = generated_only.split(stop_text)[0]\n",
    "\n",
    "        return generated_only.strip()\n",
    "\n",
    "\n",
    "def translate(sample):\n",
    "    def translate_to_english(txt):\n",
    "        lines = txt.split(\"\\n\")\n",
    "        toks, _ = tokenizer_ca_en.tokenize_batch(lines)\n",
    "        translated = ca_en_model.translate_batch(toks)\n",
    "        ts = []\n",
    "        for t in translated:\n",
    "            ts.append(tokenizer_ca_en.detokenize(t.hypotheses[0]))\n",
    "\n",
    "        return \"\\n\".join(ts)\n",
    "    en_prompt = translate_to_english(sample['prompt'])\n",
    "    en_answer = translate_to_english(sample['answer'])\n",
    "    return {\"prompt\": en_prompt, \"answer\": en_answer}\n",
    "\n",
    "\n",
    "def compute_metrics(sample):\n",
    "    prob = compute_probability(sample['prompt'], sample['answer'])\n",
    "    prediction = run_inference(sample['prompt'])\n",
    "    f1 = f1_score(prediction, sample['answer'])\n",
    "    bleu = calculate_bleu_score(prediction, sample['answer'])\n",
    "    return {\"prediction\": prediction, \"prob\": prob, \"f1\": f1, \"bleu\": bleu}\n",
    "\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "def calculate_bleu_score(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a prediction against a ground truth.\n",
    "\n",
    "    Args:\n",
    "    prediction (str): The predicted text.\n",
    "    ground_truth (str): The reference text (ground truth).\n",
    "\n",
    "    Returns:\n",
    "    float: The BLEU score.\n",
    "    \"\"\"\n",
    "    # Tokenizing the texts into words\n",
    "    prediction_tokens = word_tokenize(prediction)\n",
    "    ground_truth_tokens = [word_tokenize(ground_truth)]  # List of lists for multiple references support\n",
    "\n",
    "    # Calculating BLEU score\n",
    "    bleu_score = sentence_bleu(ground_truth_tokens, prediction_tokens)\n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d3a1bd-55a2-464a-acec-77d1aeb7a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function translate at 0x7fac1d90b370> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 10/10 [00:03<00:00,  2.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "viquiquad = load_dataset(\"data\", data_files=\"viquiquad.csv\", split=\"train[:10]\")\n",
    "viquiquad_en = viquiquad.map(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f9f86e-25f9-4544-9a35-209cff2edf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]/workspace/poligloteval/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/workspace/poligloteval/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Map: 100%|██████████| 10/10 [00:22<00:00,  2.27s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob</th>\n",
       "      <th>f1</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En aquesta època es va consolidar el concepte ...</td>\n",
       "      <td>Life, Paris-Match, Stern o Época</td>\n",
       "      <td>Paris-Match, Stern i Época</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.947126e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Després de la seva mort, s'han celebrat divers...</td>\n",
       "      <td>en un certamen de fotografia en blanc i negre ...</td>\n",
       "      <td>----</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El 1952 es va fer soci de l'Agrupació Fotogràf...</td>\n",
       "      <td>un treball sobre l'emplaçament on es construir...</td>\n",
       "      <td>----</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Durant la dècada de 1960 també va exercir de r...</td>\n",
       "      <td>viatjar per un gran nombre de països</td>\n",
       "      <td>----</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El 1957 hi va haver la primera de les dues exp...</td>\n",
       "      <td>el pas de les classes populars cap al nou ento...</td>\n",
       "      <td>----</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finalment, cal destacar que cap de les dues pr...</td>\n",
       "      <td>incòmoda</td>\n",
       "      <td>----</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Els dos relats però, no només presenten simili...</td>\n",
       "      <td>parlant amb ella</td>\n",
       "      <td>----</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Helen Fielding va crear la vida de Bridget Jon...</td>\n",
       "      <td>a través dels conflictes sentimentals en les p...</td>\n",
       "      <td>----</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No és fins un mes més tard, ja en ple Nadal, q...</td>\n",
       "      <td>Natasha</td>\n",
       "      <td>----</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L'elecció de Renée Zellweger com protagonista ...</td>\n",
       "      <td>Time</td>\n",
       "      <td>----</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  En aquesta època es va consolidar el concepte ...   \n",
       "1  Després de la seva mort, s'han celebrat divers...   \n",
       "2  El 1952 es va fer soci de l'Agrupació Fotogràf...   \n",
       "3  Durant la dècada de 1960 també va exercir de r...   \n",
       "4  El 1957 hi va haver la primera de les dues exp...   \n",
       "5  Finalment, cal destacar que cap de les dues pr...   \n",
       "6  Els dos relats però, no només presenten simili...   \n",
       "7  Helen Fielding va crear la vida de Bridget Jon...   \n",
       "8  No és fins un mes més tard, ja en ple Nadal, q...   \n",
       "9  L'elecció de Renée Zellweger com protagonista ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                   Life, Paris-Match, Stern o Época   \n",
       "1  en un certamen de fotografia en blanc i negre ...   \n",
       "2  un treball sobre l'emplaçament on es construir...   \n",
       "3               viatjar per un gran nombre de països   \n",
       "4  el pas de les classes populars cap al nou ento...   \n",
       "5                                           incòmoda   \n",
       "6                                   parlant amb ella   \n",
       "7  a través dels conflictes sentimentals en les p...   \n",
       "8                                            Natasha   \n",
       "9                                               Time   \n",
       "\n",
       "                   prediction      prob        f1          bleu  \n",
       "0  Paris-Match, Stern i Época  0.648438  0.666667  4.947126e-78  \n",
       "1                        ----  0.546875  0.000000  0.000000e+00  \n",
       "2                        ----  0.562500  0.000000  0.000000e+00  \n",
       "3                        ----  0.527344  0.000000  0.000000e+00  \n",
       "4                        ----  0.546875  0.000000  0.000000e+00  \n",
       "5                        ----  0.562500  0.000000  0.000000e+00  \n",
       "6                        ----  0.539062  0.000000  0.000000e+00  \n",
       "7                        ----  0.507812  0.000000  0.000000e+00  \n",
       "8                        ----  0.781250  0.000000  0.000000e+00  \n",
       "9                        ----  0.789062  0.000000  0.000000e+00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca = viquiquad.map(compute_metrics)\n",
    "results_ca.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a429419-652f-4d6e-a2c1-a3161f2a13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]/workspace/poligloteval/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/workspace/poligloteval/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/workspace/poligloteval/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Map: 100%|██████████| 10/10 [00:22<00:00,  2.22s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob</th>\n",
       "      <th>f1</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>During this period, the modern concept of phot...</td>\n",
       "      <td>Life, Paris-Match, Stern or Época</td>\n",
       "      <td>Paris Match, Paris Match, Paris Match, Paris M...</td>\n",
       "      <td>0.326172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100888e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After his death, several events and exhibition...</td>\n",
       "      <td>in a black and white photography contest about...</td>\n",
       "      <td>Photographic contest about Barcelona</td>\n",
       "      <td>0.589844</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.474304e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1952 he became a member of the Photographic...</td>\n",
       "      <td>a work on the site where the College of Archit...</td>\n",
       "      <td>to draw the site where the Colegio de Arquitec...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2.393949e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During the 1960s he also served as a reporter ...</td>\n",
       "      <td>travel through a large number of countries</td>\n",
       "      <td>photographic reporting</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 1957 there was the first of the two exhibit...</td>\n",
       "      <td>the shift of popular classes towards the new u...</td>\n",
       "      <td>The photographic poetics of the time</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>7.107197e-232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finally, it should be noted that neither of th...</td>\n",
       "      <td>inconvenient</td>\n",
       "      <td>Bridget Jones is aware of her own imperfection...</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The two stories, however, not only present sim...</td>\n",
       "      <td>talking to her</td>\n",
       "      <td>Mark Darcy</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Helen Fielding created Bridget Jones's life in...</td>\n",
       "      <td>through the sentimental conflicts in couples a...</td>\n",
       "      <td>through sentimental conflicts in couples and t...</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.423627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It is not until a month later, already in the ...</td>\n",
       "      <td>Natasha Natasha</td>\n",
       "      <td>Mark Darcy</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The choice of Renée Zellweger as the main char...</td>\n",
       "      <td>Time to Time</td>\n",
       "      <td>Time magazine</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9.291880e-232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  During this period, the modern concept of phot...   \n",
       "1  After his death, several events and exhibition...   \n",
       "2  In 1952 he became a member of the Photographic...   \n",
       "3  During the 1960s he also served as a reporter ...   \n",
       "4  In 1957 there was the first of the two exhibit...   \n",
       "5  Finally, it should be noted that neither of th...   \n",
       "6  The two stories, however, not only present sim...   \n",
       "7  Helen Fielding created Bridget Jones's life in...   \n",
       "8  It is not until a month later, already in the ...   \n",
       "9  The choice of Renée Zellweger as the main char...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                  Life, Paris-Match, Stern or Época   \n",
       "1  in a black and white photography contest about...   \n",
       "2  a work on the site where the College of Archit...   \n",
       "3         travel through a large number of countries   \n",
       "4  the shift of popular classes towards the new u...   \n",
       "5                                       inconvenient   \n",
       "6                                     talking to her   \n",
       "7  through the sentimental conflicts in couples a...   \n",
       "8                                    Natasha Natasha   \n",
       "9                                       Time to Time   \n",
       "\n",
       "                                          prediction      prob        f1  \\\n",
       "0  Paris Match, Paris Match, Paris Match, Paris M...  0.326172  0.000000   \n",
       "1               Photographic contest about Barcelona  0.589844  0.500000   \n",
       "2  to draw the site where the Colegio de Arquitec...  0.750000  0.416667   \n",
       "3                             photographic reporting  0.447266  0.000000   \n",
       "4               The photographic poetics of the time  0.279297  0.166667   \n",
       "5  Bridget Jones is aware of her own imperfection...  0.503906  0.000000   \n",
       "6                                         Mark Darcy  0.687500  0.000000   \n",
       "7  through sentimental conflicts in couples and t...  0.683594  1.000000   \n",
       "8                                         Mark Darcy  0.431641  0.000000   \n",
       "9                                      Time magazine  0.492188  0.400000   \n",
       "\n",
       "            bleu  \n",
       "0  1.100888e-231  \n",
       "1   2.474304e-78  \n",
       "2   2.393949e-01  \n",
       "3   0.000000e+00  \n",
       "4  7.107197e-232  \n",
       "5   0.000000e+00  \n",
       "6   0.000000e+00  \n",
       "7   8.423627e-01  \n",
       "8   0.000000e+00  \n",
       "9  9.291880e-232  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_en = viquiquad_en.map(compute_metrics)\n",
    "results_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 261.31ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 362.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12753"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca.to_csv(f\"results/{model_name}-viquiquad-ca.csv\", index=False)\n",
    "results_en.to_csv(f\"results/{model_name}-viquiquad-en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob    6.011719e-01\n",
      "f1      6.666667e-02\n",
      "bleu    4.947126e-79\n",
      "dtype: float64\n",
      "prob    0.519141\n",
      "f1      0.248333\n",
      "bleu    0.108176\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_ca_mean = results_ca.to_pandas()[['prob', 'f1', 'bleu']].mean()\n",
    "results_en_mean = results_en.to_pandas()[['prob', 'f1', 'bleu']].mean()\n",
    "print(results_ca_mean)\n",
    "print(results_en_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
