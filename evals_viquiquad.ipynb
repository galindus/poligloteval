{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:15<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"projecte-aina/aguila-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split('/')[1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac21dd30-0551-43f9-8d8a-5b04df3a59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 16539.05it/s]\n"
     ]
    }
   ],
   "source": [
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 1\n",
    "    with torch.no_grad():\n",
    "        for token in answer_tokens:\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # Logits are in the outputs, you can access the last token's logits like this:\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            #probabilities = min_max_scaling(logits)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            answer_probability *= probabilities[0][token]\n",
    "\n",
    "            # Prepare input_ids for the next token prediction\n",
    "            new_token = torch.tensor([[token]]).to(model.device)\n",
    "            inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                    'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "            del new_token\n",
    "        del inputs\n",
    "    return answer_probability.item()\n",
    "\n",
    "\n",
    "def run_inference(txt, num_tokens=20, stop_text='\\n'):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    # Calculate the total length of the output (input length + number of tokens to generate)\n",
    "    max_length = len(inputs['input_ids'][0]) + num_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens\n",
    "        tokens = model.generate(**inputs, do_sample=True, top_k=50, eos_token_id=tokenizer.eos_token_id, max_length=max_length)\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Slice the generated text to exclude the input prompt\n",
    "        generated_only = generated_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):]\n",
    "\n",
    "        # If a stop text is found, truncate the output at its first occurrence\n",
    "        if stop_text in generated_only:\n",
    "            generated_only = generated_only.split(stop_text)[0]\n",
    "\n",
    "        return generated_only.strip()\n",
    "\n",
    "\n",
    "def translate(sample):\n",
    "    def translate_to_english(txt):\n",
    "        lines = txt.split(\"\\n\")\n",
    "        toks, _ = tokenizer_ca_en.tokenize_batch(lines)\n",
    "        translated = ca_en_model.translate_batch(toks)\n",
    "        ts = []\n",
    "        for t in translated:\n",
    "            ts.append(tokenizer_ca_en.detokenize(t.hypotheses[0]))\n",
    "\n",
    "        return \"\\n\".join(ts)\n",
    "    en_prompt = translate_to_english(sample['prompt'])\n",
    "    en_answer = translate_to_english(sample['answer'])\n",
    "    return {\"prompt\": en_prompt, \"answer\": en_answer}\n",
    "\n",
    "\n",
    "def compute_metrics(sample):\n",
    "    prob = compute_probability(sample['prompt'], sample['answer'])\n",
    "    prediction = run_inference(sample['prompt'])\n",
    "    f1 = f1_score(prediction, sample['answer'])\n",
    "    return {\"prediction\": prediction, \"prob\": prob, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "79d3a1bd-55a2-464a-acec-77d1aeb7a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2475 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2475/2475 [15:23<00:00,  2.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "viquiquad = load_dataset(\"data\", data_files=\"viquiquad.csv\", split=\"train\")\n",
    "viquiquad_en = viquiquad.map(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22f9f86e-25f9-4544-9a35-209cff2edf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2475 [00:00<?, ? examples/s]/workspace/poligloteval/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Map:  14%|█▎        | 337/2475 [11:20<1:09:01,  1.94s/ examples]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  68%|██████▊   | 1686/2475 [55:11<25:03,  1.91s/ examples]  "
     ]
    }
   ],
   "source": [
    "results_ca = viquiquad.map(compute_metrics)\n",
    "results_ca.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a429419-652f-4d6e-a2c1-a3161f2a13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  50%|█████     | 1/2 [00:07<00:07,  7.18s/ examples]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map: 100%|██████████| 2/2 [00:19<00:00,  9.99s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>During this period, the modern concept of phot...</td>\n",
       "      <td>Life, Paris-Match, Stern or Época</td>\n",
       "      <td>Epoclendor, Paris</td>\n",
       "      <td>2.733924e-15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After his death, several events and exhibition...</td>\n",
       "      <td>in a black and white photography contest about...</td>\n",
       "      <td>photography about Barceló</td>\n",
       "      <td>6.810296e-09</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  During this period, the modern concept of phot...   \n",
       "1  After his death, several events and exhibition...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                  Life, Paris-Match, Stern or Época   \n",
       "1  in a black and white photography contest about...   \n",
       "\n",
       "                  prediction          prob        f1  \n",
       "0          Epoclendor, Paris  2.733924e-15  0.000000  \n",
       "1  photography about Barceló  6.810296e-09  0.363636  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_en = viquiquad_en.map(compute_metrics)\n",
    "results_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 399.31ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 550.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2265"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca.to_csv(f\"results/{model_name}-viquiquad-ca.csv\", index=False)\n",
    "results_en.to_csv(f\"results/{model_name}-viquiquad-en.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
