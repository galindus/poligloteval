{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2\n",
    "from metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644a24d23e1845868c47db202c6b3280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b437fdc7ac5f4ec78794c509a468cdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"projecte-aina/aguila-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split('/')[1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 0\n",
    "    with torch.no_grad():\n",
    "        for token in answer_tokens:\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # Logits are in the outputs, you can access the last token's logits like this:\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            log_probs = torch.log(min_max_scaling(logits))\n",
    "            answer_probability += log_probs[0][token]\n",
    "\n",
    "            # Prepare input_ids for the next token prediction\n",
    "            new_token = torch.tensor([[token]]).to(model.device)\n",
    "            inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                    'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "    return torch.exp(answer_probability).item()\n",
    "    # return answer_probability.item()\n",
    "\n",
    "\n",
    "def run_inference(txt, num_tokens=20, stop_text='\\n'):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(txt, return_tensors=\"pt\").to(model.device)['input_ids']\n",
    "    # Calculate the total length of the output (input length + number of tokens to generate)\n",
    "\n",
    "    generated_text = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens\n",
    "        for _ in range(num_tokens):\n",
    "            max_length = len(tokens[0]) + 1\n",
    "            tokens = model.generate(tokens, do_sample=True, top_k=1, eos_token_id=tokenizer.eos_token_id, max_length=max_length)\n",
    "\n",
    "            # Decode the generated tokens into text\n",
    "            generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            # If a stop text is found, truncate the output at its first occurrence\n",
    "            if stop_text is not None:\n",
    "                if generated_text[-len(stop_text):] == stop_text:\n",
    "                    break\n",
    "\n",
    "        generated_only = generated_text.replace(txt, \"\").strip()\n",
    "        return generated_only\n",
    "\n",
    "\n",
    "def translate(sample):\n",
    "    def translate_to_english(txt):\n",
    "        lines = [l for l in txt.split(\"\\n\") if l.strip() != \"\"]\n",
    "\n",
    "        toks, _ = tokenizer_ca_en.tokenize_batch(lines)\n",
    "        translated = ca_en_model.translate_batch(toks)\n",
    "        ts = []\n",
    "        for t in translated:\n",
    "            t_str = tokenizer_ca_en.detokenize(t.hypotheses[0])\n",
    "            # That is a bug on the translation outputing twice the translation.\n",
    "            if len(txt.split(\" \")) == 1 and len(t_str.split(\" \")) == 2:\n",
    "                t_str = t_str.split(\" \")[0]\n",
    "            ts.append(t_str)\n",
    "\n",
    "        return \"\\n\".join(ts)\n",
    "    en_prompt = translate_to_english(sample['prompt'])\n",
    "    en_answer = translate_to_english(sample['answer'])\n",
    "    return {\"prompt\": en_prompt, \"answer\": en_answer}\n",
    "\n",
    "\n",
    "def compute_metrics(sample):\n",
    "    prob = compute_probability(sample['prompt'], sample['answer'])\n",
    "    prediction = run_inference(sample['prompt'])\n",
    "    f1 = f1_score(prediction, sample['answer'])\n",
    "    bleu = calculate_bleu_score(prediction, sample['answer'])\n",
    "    return {\"prediction\": prediction, \"prob\": prob, \"f1\": f1, \"bleu\": bleu}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cae7642-5720-440f-a939-2f4297da159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a148a167e52447df9e47972e66eda342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b2d02aa6e943df839c54023612ed65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49b4028e6324ddf8dbadb2edd32519f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function translate at 0x7f76c81bc040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e7b0798ea847b4b18ba02571cdc472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teca = load_dataset(\"data\", data_files=\"teca.csv\", split=\"train[:10]\")\n",
    "teca_en = teca.map(translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78758167-caca-47f0-8d01-fdae993b0528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be39ee0932b74533a189c485558fc3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>numeric_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premissa: Nosaltres, per exemple, no estem d'a...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>realitat</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>premissa: Van rebre la visita d'un altre centr...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>neutre</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>premissa: Els CDR convoquen una protesta davan...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>premissa: L’Espai LGTBI de Girona denuncia un ...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>premissa: En aquesta ocasió, compten amb els m...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>premissa: Senda ofereix pràctiques d'un períod...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>premissa: El TC admet la impugnació del govern...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>victòria</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>premissa: Si descrivim i compararem, els parla...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>premissa: El doctor Josep Morata Socias i la p...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>llengua</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>premissa: Això és el que passa quan la teva ge...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>afirmació</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt      answer  \\\n",
       "0  premissa: Nosaltres, per exemple, no estem d'a...  implicació   \n",
       "1  premissa: Van rebre la visita d'un altre centr...  implicació   \n",
       "2  premissa: Els CDR convoquen una protesta davan...  implicació   \n",
       "3  premissa: L’Espai LGTBI de Girona denuncia un ...      neutre   \n",
       "4  premissa: En aquesta ocasió, compten amb els m...      neutre   \n",
       "5  premissa: Senda ofereix pràctiques d'un períod...  implicació   \n",
       "6  premissa: El TC admet la impugnació del govern...      neutre   \n",
       "7  premissa: Si descrivim i compararem, els parla...      neutre   \n",
       "8  premissa: El doctor Josep Morata Socias i la p...      neutre   \n",
       "9  premissa: Això és el que passa quan la teva ge...      neutre   \n",
       "\n",
       "   numeric_label    prediction  results  \n",
       "0              0      realitat    False  \n",
       "1              0        neutre    False  \n",
       "2              0  contradicció    False  \n",
       "3              1  contradicció    False  \n",
       "4              1  contradicció    False  \n",
       "5              0  contradicció    False  \n",
       "6              1      victòria    False  \n",
       "7              1  contradicció    False  \n",
       "8              1       llengua    False  \n",
       "9              1     afirmació    False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval(entry):\n",
    "    prediction = run_inference(entry['prompt'], num_tokens=20)\n",
    "    return { 'prediction': prediction, 'results': prediction == str(entry['answer']) }\n",
    "\n",
    "results_ca = teca.map(eval)\n",
    "results_ca.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f8957a2a314f6bbc0107943bc141b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>numeric_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premise: We, for example, do not agree to be t...</td>\n",
       "      <td>involvement</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premise: They received the visit of another ce...</td>\n",
       "      <td>involvement</td>\n",
       "      <td>0</td>\n",
       "      <td>Colau</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>premise: The CDRs call for a protest in front ...</td>\n",
       "      <td>involvement</td>\n",
       "      <td>0</td>\n",
       "      <td>21-D</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>premise: Espai LGTBI de Girona denounces a fas...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>independentisme</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Premise: On this occasion, they have the music...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Premise: Senda offers internships for a minimu...</td>\n",
       "      <td>involvement</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Premise: The TC admits the challenge of the go...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Premise: If we describe and compare, the diale...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>premise: Dr. Josep Morata Socias and Professor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>premise: This is what happens when your sister...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>Bulgarian music</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt       answer  \\\n",
       "0  Premise: We, for example, do not agree to be t...  involvement   \n",
       "1  Premise: They received the visit of another ce...  involvement   \n",
       "2  premise: The CDRs call for a protest in front ...  involvement   \n",
       "3  premise: Espai LGTBI de Girona denounces a fas...      neutral   \n",
       "4  Premise: On this occasion, they have the music...      neutral   \n",
       "5  Premise: Senda offers internships for a minimu...  involvement   \n",
       "6  Premise: The TC admits the challenge of the go...      neutral   \n",
       "7  Premise: If we describe and compare, the diale...      neutral   \n",
       "8  premise: Dr. Josep Morata Socias and Professor...      neutral   \n",
       "9  premise: This is what happens when your sister...      neutral   \n",
       "\n",
       "   numeric_label       prediction  results  \n",
       "0              0                     False  \n",
       "1              0            Colau    False  \n",
       "2              0             21-D    False  \n",
       "3              1  independentisme    False  \n",
       "4              1                     False  \n",
       "5              0                     False  \n",
       "6              1          Uruguay    False  \n",
       "7              1                     False  \n",
       "8              1                     False  \n",
       "9              1  Bulgarian music    False  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_en = teca_en.map(eval)\n",
    "results_en.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8818191-c389-41b4-8f1c-0cac80484cc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/aguila-7b-teca-ca.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/poligloteval/evals_teca.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/poligloteval/evals_teca.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m results_ca\u001b[39m.\u001b[39;49mto_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresults/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m-teca-ca.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/poligloteval/evals_teca.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m results_en\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m-teca-en.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/poligloteval/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:4743\u001b[0m, in \u001b[0;36mDataset.to_csv\u001b[0;34m(self, path_or_buf, batch_size, num_proc, **to_csv_kwargs)\u001b[0m\n\u001b[1;32m   4740\u001b[0m \u001b[39m# Dynamic import to avoid circular dependency\u001b[39;00m\n\u001b[1;32m   4741\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcsv\u001b[39;00m \u001b[39mimport\u001b[39;00m CsvDatasetWriter\n\u001b[0;32m-> 4743\u001b[0m \u001b[39mreturn\u001b[39;00m CsvDatasetWriter(\u001b[39mself\u001b[39;49m, path_or_buf, batch_size\u001b[39m=\u001b[39;49mbatch_size, num_proc\u001b[39m=\u001b[39;49mnum_proc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mto_csv_kwargs)\u001b[39m.\u001b[39;49mwrite()\n",
      "File \u001b[0;32m/workspace/poligloteval/venv/lib/python3.10/site-packages/datasets/io/csv.py:93\u001b[0m, in \u001b[0;36mCsvDatasetWriter.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_csv_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_or_buf, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m, os\u001b[39m.\u001b[39mPathLike)):\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_or_buf, \u001b[39m\"\u001b[39;49m\u001b[39mwb+\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m buffer:\n\u001b[1;32m     94\u001b[0m         written \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write(file_obj\u001b[39m=\u001b[39mbuffer, header\u001b[39m=\u001b[39mheader, index\u001b[39m=\u001b[39mindex, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_csv_kwargs)\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/aguila-7b-teca-ca.csv'"
     ]
    }
   ],
   "source": [
    "results_ca.to_csv(f\"results/{model_name}-teca-ca.csv\", index=False)\n",
    "results_en.to_csv(f\"results/{model_name}-teca-en.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct ca: 0.0\n",
      "Percentage correct en: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage correct ca:\", sum(results_ca['results']) / len(results_ca))\n",
    "print(\"Percentage correct en:\", sum(results_en['results']) / len(results_en))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
