{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/poligloteval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 23147.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"projecte-aina/aguila-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split('/')[1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 1\n",
    "    with torch.no_grad():\n",
    "        for token in answer_tokens:\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # Logits are in the outputs, you can access the last token's logits like this:\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "            answer_probability += log_probs[0][token]\n",
    "\n",
    "            # Prepare input_ids for the next token prediction\n",
    "            new_token = torch.tensor([[token]]).to(model.device)\n",
    "            inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                    'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "    return torch.exp(answer_probability).item()\n",
    "\n",
    "\n",
    "def run_inference(txt, num_tokens=20, stop_text='\\n'):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    # Calculate the total length of the output (input length + number of tokens to generate)\n",
    "    max_length = len(inputs['input_ids'][0]) + num_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens\n",
    "        tokens = model.generate(**inputs, do_sample=True, top_k=1, eos_token_id=tokenizer.eos_token_id, max_length=max_length)\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Slice the generated text to exclude the input prompt\n",
    "        generated_only = generated_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):]\n",
    "\n",
    "        # If a stop text is found, truncate the output at its first occurrence\n",
    "        if stop_text in generated_only:\n",
    "            generated_only = generated_only.split(stop_text)[0]\n",
    "\n",
    "        return generated_only.strip()\n",
    "\n",
    "\n",
    "def translate(sample):\n",
    "    def translate_to_english(txt):\n",
    "        lines = txt.split(\"\\n\")\n",
    "        toks, _ = tokenizer_ca_en.tokenize_batch(lines)\n",
    "        translated = ca_en_model.translate_batch(toks)\n",
    "        ts = []\n",
    "        for t in translated:\n",
    "            ts.append(tokenizer_ca_en.detokenize(t.hypotheses[0]))\n",
    "\n",
    "        return \"\\n\".join(ts)\n",
    "    en_prompt = translate_to_english(sample['prompt'])\n",
    "    en_answer = translate_to_english(sample['answer'])\n",
    "    return {\"prompt\": en_prompt, \"answer\": en_answer}\n",
    "\n",
    "\n",
    "def compute_metrics(sample):\n",
    "    prob = compute_probability(sample['prompt'], sample['answer'])\n",
    "    prediction = run_inference(sample['prompt'])\n",
    "    f1 = f1_score(prediction, sample['answer'])\n",
    "    return {\"prediction\": prediction, \"prob\": prob, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cae7642-5720-440f-a939-2f4297da159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function translate at 0x7fc142290ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:  10%|█         | 1/10 [00:00<00:01,  5.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:01<00:00,  7.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "teca = load_dataset(\"data\", data_files=\"teca.csv\", split=\"train[:10]\")\n",
    "teca_en = teca.map(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78758167-caca-47f0-8d01-fdae993b0528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]/workspace/poligloteval/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10/10 [00:15<00:00,  1.57s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>numeric_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premissa: Nosaltres, per exemple, no estem d'a...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>----\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>premissa: Els CDR convoquen una protesta davan...</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>2</td>\n",
       "      <td>----\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>premissa: Vaig incorporar me a la feina quan e...</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>2</td>\n",
       "      <td>----\\n\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>premissa: La tradició i la modernitat conviuen...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>----\\n\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>premissa: Senda ofereix pràctiques d'un períod...</td>\n",
       "      <td>implicació</td>\n",
       "      <td>0</td>\n",
       "      <td>----\\n\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>premissa: El doctor Josep Morata Socias i la p...</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>2</td>\n",
       "      <td>----------\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>premissa: Això és el que passa quan la teva ge...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>premissa: M'ha sabut molt greu però no hi he p...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>?natural\\n----\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>premissa: El Concurs ha aconseguit gran presti...</td>\n",
       "      <td>neutre</td>\n",
       "      <td>1</td>\n",
       "      <td>----\\n\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>premissa: Però l'altre dia llegint a la biblio...</td>\n",
       "      <td>contradicció</td>\n",
       "      <td>2</td>\n",
       "      <td>----------\\n\\npremissa:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt        answer  \\\n",
       "0  premissa: Nosaltres, per exemple, no estem d'a...    implicació   \n",
       "1  premissa: Els CDR convoquen una protesta davan...  contradicció   \n",
       "2  premissa: Vaig incorporar me a la feina quan e...  contradicció   \n",
       "3  premissa: La tradició i la modernitat conviuen...        neutre   \n",
       "4  premissa: Senda ofereix pràctiques d'un períod...    implicació   \n",
       "5  premissa: El doctor Josep Morata Socias i la p...  contradicció   \n",
       "6  premissa: Això és el que passa quan la teva ge...        neutre   \n",
       "7  premissa: M'ha sabut molt greu però no hi he p...        neutre   \n",
       "8  premissa: El Concurs ha aconseguit gran presti...        neutre   \n",
       "9  premissa: Però l'altre dia llegint a la biblio...  contradicció   \n",
       "\n",
       "   numeric_label                 prediction  results  \n",
       "0              0            ----\\npremissa:    False  \n",
       "1              2            ----\\npremissa:    False  \n",
       "2              2          ----\\n\\npremissa:    False  \n",
       "3              1          ----\\n\\npremissa:    False  \n",
       "4              0          ----\\n\\npremissa:    False  \n",
       "5              2      ----------\\npremissa:    False  \n",
       "6              1                               False  \n",
       "7              1  ?natural\\n----\\npremissa:    False  \n",
       "8              1          ----\\n\\npremissa:    False  \n",
       "9              2    ----------\\n\\npremissa:    False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval(entry):\n",
    "    prediction = run_inference(entry['prompt'], num_tokens=20, stop_text=' ')\n",
    "    return { 'prediction': prediction, 'results': prediction == str(entry['answer']) }\n",
    "\n",
    "results_ca = teca.map(eval)\n",
    "results_ca.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:16<00:00,  1.60s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>numeric_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premise: We, for example, do not agree to be t...</td>\n",
       "      <td>involvement in</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>premise: The CDRs call for a protest in front ...</td>\n",
       "      <td>contradiction in the law</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premise: I joined the work when it was decided...</td>\n",
       "      <td>contradiction in the law</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>premise: Tradition and modernity coexist in th...</td>\n",
       "      <td>neutral neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Premise: Senda offers internships for a minimu...</td>\n",
       "      <td>involvement in</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>premise: Dr. Josep Morata Socias and Professor...</td>\n",
       "      <td>contradiction in the law</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>premise: This is what happens when your sister...</td>\n",
       "      <td>neutral neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Premise: I was very sorry but I could not do a...</td>\n",
       "      <td>neutral neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Premise: The Contest has achieved great presti...</td>\n",
       "      <td>neutral neutral</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Premise: But the other day reading in the libr...</td>\n",
       "      <td>contradiction in the law</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Premise: We, for example, do not agree to be t...   \n",
       "1  premise: The CDRs call for a protest in front ...   \n",
       "2  Premise: I joined the work when it was decided...   \n",
       "3  premise: Tradition and modernity coexist in th...   \n",
       "4  Premise: Senda offers internships for a minimu...   \n",
       "5  premise: Dr. Josep Morata Socias and Professor...   \n",
       "6  premise: This is what happens when your sister...   \n",
       "7  Premise: I was very sorry but I could not do a...   \n",
       "8  Premise: The Contest has achieved great presti...   \n",
       "9  Premise: But the other day reading in the libr...   \n",
       "\n",
       "                     answer  numeric_label prediction  results  \n",
       "0            involvement in              0          -    False  \n",
       "1  contradiction in the law              2               False  \n",
       "2  contradiction in the law              2               False  \n",
       "3           neutral neutral              1          -    False  \n",
       "4            involvement in              0          -    False  \n",
       "5  contradiction in the law              2               False  \n",
       "6           neutral neutral              1               False  \n",
       "7           neutral neutral              1               False  \n",
       "8           neutral neutral              1               False  \n",
       "9  contradiction in the law              2          -    False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_en = teca_en.map(eval)\n",
    "results_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8818191-c389-41b4-8f1c-0cac80484cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 234.36ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 377.29ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12089"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ca.to_csv(f\"results/{model_name}-teca-ca.csv\", index=False)\n",
    "results_en.to_csv(f\"results/{model_name}-teca-en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct ca: 0.0\n",
      "Percentage correct en: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage correct ca:\", sum(results_ca['results']) / len(results_ca))\n",
    "print(\"Percentage correct en:\", sum(results_en['results']) / len(results_en))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
