{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb9153a-12c7-431b-9525-e9fd510d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XDG_CACHE=/workspace/.cache\n",
      "env: HF_HOME=/workspace/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "%env XDG_CACHE=/workspace/.cache\n",
    "%env HF_HOME=/workspace/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43381932-7061-42a9-ade7-27eac3938b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyonmttok\n",
    "import ctranslate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175a0fa0-01d3-4fca-8bcc-fab926fbcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translator Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 6472.69it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "## Lets Do the translation layer\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"Loading translator Models...\")\n",
    "\n",
    "ca_en_model_folder = snapshot_download(repo_id=\"projecte-aina/mt-aina-ca-en\", revision=\"main\")\n",
    "tokenizer_ca_en = pyonmttok.Tokenizer(\n",
    "    mode=\"none\", sp_model_path=ca_en_model_folder + \"/spm.model\"\n",
    ")\n",
    "ca_en_model = ctranslate2.Translator(ca_en_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8e8d1f-b662-4580-80ff-ff70e95e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probability(input_text, answer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    answer_tokens = tokenizer(answer)['input_ids']\n",
    "    answer_probability = 1\n",
    "\n",
    "    for token in answer_tokens:\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "        # Logits are in the outputs, you can access the last token's logits like this:\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probabilities = torch.softmax(logits, 1)\n",
    "        answer_probability *= probabilities[0][token]\n",
    "        \n",
    "        # Prepare input_ids for the next token prediction\n",
    "        new_token = torch.tensor([[token]]).to(model.device)\n",
    "        inputs = {'input_ids': torch.cat([inputs['input_ids'], new_token], dim=1),\n",
    "                'attention_mask': torch.cat([inputs['attention_mask'], torch.tensor([[1]]).to(model.device)], dim=1)}\n",
    "        del new_token\n",
    "    del inputs\n",
    "    return answer_probability.item()\n",
    "\n",
    "def run_inference(txt, num_tokens=20):\n",
    "    inputs = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = model.generate(**inputs, do_sample=True,\n",
    "        top_k=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=num_tokens)\n",
    "    del inputs\n",
    "    return tokenizer.decode(tokens[0]).replace(txt, \"\")\n",
    "\n",
    "def translate_to_english(txt):\n",
    "    lines = txt.split(\"\\n\")\n",
    "    translated_lines = []\n",
    "    for line in lines:\n",
    "        toks, _ = tokenizer_ca_en.tokenize(line)\n",
    "        translated = ca_en_model.translate_batch([toks])\n",
    "        translated = tokenizer_ca_en.detokenize(translated[0].hypotheses[0])\n",
    "        translated_lines.append(translated)\n",
    "        \n",
    "    return \"\\n\".join(translated_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cae7642-5720-440f-a939-2f4297da159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teca = load_dataset(\"benchmarks\", data_files=\"teca.csv\", split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78758167-caca-47f0-8d01-fdae993b0528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:   2%|▏         | 2/100 [00:00<00:08, 11.59 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:   4%|▍         | 4/100 [00:00<00:08, 11.38 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:   6%|▌         | 6/100 [00:00<00:08, 11.41 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:   8%|▊         | 8/100 [00:00<00:07, 11.54 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  10%|█         | 10/100 [00:00<00:07, 11.42 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  12%|█▏        | 12/100 [00:01<00:07, 11.58 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  14%|█▍        | 14/100 [00:01<00:07, 11.54 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  16%|█▌        | 16/100 [00:01<00:07, 11.61 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  18%|█▊        | 18/100 [00:01<00:07, 11.50 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  20%|██        | 20/100 [00:01<00:06, 11.46 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  22%|██▏       | 22/100 [00:01<00:06, 11.53 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  24%|██▍       | 24/100 [00:02<00:06, 11.77 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  26%|██▌       | 26/100 [00:02<00:06, 11.67 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  28%|██▊       | 28/100 [00:02<00:06, 11.74 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  30%|███       | 30/100 [00:02<00:05, 11.67 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  32%|███▏      | 32/100 [00:02<00:05, 11.55 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  34%|███▍      | 34/100 [00:02<00:05, 11.45 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  36%|███▌      | 36/100 [00:03<00:05, 11.53 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  38%|███▊      | 38/100 [00:03<00:05, 11.54 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  40%|████      | 40/100 [00:03<00:05, 11.49 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  42%|████▏     | 42/100 [00:03<00:05, 11.30 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  44%|████▍     | 44/100 [00:03<00:04, 11.66 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  46%|████▌     | 46/100 [00:03<00:04, 11.68 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  48%|████▊     | 48/100 [00:04<00:04, 11.56 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  50%|█████     | 50/100 [00:04<00:04, 11.70 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  52%|█████▏    | 52/100 [00:04<00:04, 11.62 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  54%|█████▍    | 54/100 [00:04<00:03, 11.50 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  56%|█████▌    | 56/100 [00:04<00:03, 11.48 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  58%|█████▊    | 58/100 [00:05<00:03, 11.40 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  60%|██████    | 60/100 [00:05<00:03, 11.51 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  62%|██████▏   | 62/100 [00:05<00:03, 11.45 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  64%|██████▍   | 64/100 [00:05<00:03, 11.36 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  66%|██████▌   | 66/100 [00:05<00:02, 11.47 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  68%|██████▊   | 68/100 [00:05<00:02, 11.42 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  70%|███████   | 70/100 [00:06<00:02, 11.37 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  72%|███████▏  | 72/100 [00:06<00:02, 11.35 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  74%|███████▍  | 74/100 [00:06<00:02, 11.63 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  76%|███████▌  | 76/100 [00:06<00:02, 11.75 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  78%|███████▊  | 78/100 [00:06<00:01, 11.57 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  80%|████████  | 80/100 [00:06<00:01, 11.84 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  82%|████████▏ | 82/100 [00:07<00:01, 11.86 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  84%|████████▍ | 84/100 [00:07<00:01, 11.81 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  86%|████████▌ | 86/100 [00:07<00:01, 11.67 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  88%|████████▊ | 88/100 [00:07<00:01, 11.96 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  90%|█████████ | 90/100 [00:07<00:00, 11.68 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  92%|█████████▏| 92/100 [00:07<00:00, 11.89 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  94%|█████████▍| 94/100 [00:08<00:00, 12.09 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  96%|█████████▌| 96/100 [00:08<00:00, 12.02 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map:  98%|█████████▊| 98/100 [00:08<00:00, 11.86 examples/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Map: 100%|██████████| 100/100 [00:08<00:00, 11.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def eval(entry):\n",
    "    return { 'results': run_inference(entry['prompt'], 1) == str(entry['numeric_label']) }\n",
    "\n",
    "results = teca.map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e75480a1-1d3c-4dab-90fb-9210c70c9ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8818191-c389-41b4-8f1c-0cac80484cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
